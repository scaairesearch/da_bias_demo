{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyOVszLerjkNp7qqfnnX4CBM",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/scaairesearch/da_bias_demo/blob/main/Gradio_v1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Demo app to mitigate bias"
      ],
      "metadata": {
        "id": "uK0Gu9LvnWTC"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "zliQE7LckwrD"
      },
      "outputs": [],
      "source": [
        "!pip install --quiet facenet-pytorch gradio"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --quiet --upgrade Pillow"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NzoGezkJ2GZs",
        "outputId": "7f1429f4-4450-4c35-ca49-348724285ef8"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "facenet-pytorch 2.6.0 requires Pillow<10.3.0,>=10.2.0, but you have pillow 10.4.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import IPython\n",
        "\n",
        "# Save the current notebook and restart the kernel\n",
        "def restart_kernel():\n",
        "    os.system(\"touch /tmp/restart-kernel\")\n",
        "    IPython.display.display(IPython.display.Javascript('IPython.notebook.save_checkpoint();'))\n",
        "    IPython.display.display(IPython.display.Javascript('IPython.notebook.kernel.restart();'))\n",
        "\n",
        "restart_kernel()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 17
        },
        "id": "QFSRi--q2tfT",
        "outputId": "1d27846a-ccc0-4965-ae17-1c99515dc99d"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "IPython.notebook.save_checkpoint();"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "IPython.notebook.kernel.restart();"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# install general libraries\n",
        "import os\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt # for plots\n",
        "\n",
        "import time\n",
        "from pathlib import Path\n",
        "\n",
        "# from copy import deepcopy\n",
        "import numpy as np\n",
        "\n",
        "# from sklearn.metrics import accuracy_score, precision_score, confusion_matrix, classification_report\n",
        "import pandas as pd\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from pathlib import Path\n",
        "from torchvision import transforms\n",
        "import torch\n",
        "from PIL import Image\n",
        "from facenet_pytorch import MTCNN\n",
        "\n",
        "from torch.autograd import Function\n",
        "import torch.nn.functional as F\n",
        "import gradio as gr\n",
        "from collections import OrderedDict\n",
        "from copy import deepcopy\n",
        "import cv2\n"
      ],
      "metadata": {
        "id": "sedKbq1SlJpt"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# check for cuda\n",
        "cuda = torch.cuda.is_available()\n",
        "print (f' Cuda Status : {cuda}')\n",
        "\n",
        "# setting seed\n",
        "SEED = 42 # arbit seed, why 42 - because in hitch hikers guide to galaxy it is answer to everything\n",
        "# torch.cuda.seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED) if cuda else torch.manual_seed(SEED)\n",
        "\n",
        "device = torch.device(\"cuda\" if cuda else \"cpu\")\n",
        "print(device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1nB9cD9ylzux",
        "outputId": "bc822e3e-9109-47ee-e424-6254a959a2a9"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " Cuda Status : False\n",
            "cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "mtcnn = MTCNN(image_size=224, device= device) # default is 224, now no need to mention later on"
      ],
      "metadata": {
        "id": "q7qTuuf_l0kr"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Mounting gdrive"
      ],
      "metadata": {
        "id": "lDWXPeKVpaZU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the mounted drive and mounting if not done\n",
        "if not os.path.exists('/content/gdrive'):\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "else:\n",
        "    print(\"Google Drive is already mounted.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_s97m8UdpaKc",
        "outputId": "5b372e8c-aaa9-4f4e-fb1a-147ce5cdb871"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Google Drive is already mounted.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load Models"
      ],
      "metadata": {
        "id": "dN201qexpP4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GDRIVE_FOLDER = '/content/gdrive/MyDrive/CV_FER'"
      ],
      "metadata": {
        "id": "pzlnOextpZW4"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Network - Pretrained"
      ],
      "metadata": {
        "id": "Qod56fDMpo_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Vgg_vd_face_sfew_dag(nn.Module):\n",
        "\n",
        "    def __init__(self):\n",
        "        super(Vgg_vd_face_sfew_dag, self).__init__()\n",
        "        self.meta = {'mean': [129.186279296875, 104.76238250732422, 93.59396362304688],\n",
        "                     'std': [1, 1, 1],\n",
        "                     'imageSize': [224, 224, 3]}\n",
        "        self.conv1_1 = nn.Conv2d(3, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu1_1 = nn.ReLU()\n",
        "        self.conv1_2 = nn.Conv2d(64, 64, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu1_2 = nn.ReLU()\n",
        "        self.pool1 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
        "        self.conv2_1 = nn.Conv2d(64, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu2_1 = nn.ReLU()\n",
        "        self.conv2_2 = nn.Conv2d(128, 128, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu2_2 = nn.ReLU()\n",
        "        self.pool2 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
        "        self.conv3_1 = nn.Conv2d(128, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu3_1 = nn.ReLU()\n",
        "        self.conv3_2 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu3_2 = nn.ReLU()\n",
        "        self.conv3_3 = nn.Conv2d(256, 256, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu3_3 = nn.ReLU()\n",
        "        self.pool3 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
        "        self.conv4_1 = nn.Conv2d(256, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu4_1 = nn.ReLU()\n",
        "        self.conv4_2 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu4_2 = nn.ReLU()\n",
        "        self.conv4_3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu4_3 = nn.ReLU()\n",
        "        self.pool4 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
        "        self.conv5_1 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu5_1 = nn.ReLU()\n",
        "        self.conv5_2 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu5_2 = nn.ReLU()\n",
        "        self.conv5_3 = nn.Conv2d(512, 512, kernel_size=[3, 3], stride=(1, 1), padding=(1, 1))\n",
        "        self.relu5_3 = nn.ReLU()\n",
        "        self.pool5 = nn.MaxPool2d(kernel_size=[2, 2], stride=[2, 2], padding=0, dilation=1, ceil_mode=False)\n",
        "        self.fc6 = nn.Conv2d(512, 4096, kernel_size=[7, 7], stride=(1, 1))\n",
        "        self.relu6 = nn.ReLU()\n",
        "        self.fc7 = nn.Linear(in_features=4096, out_features=4096, bias=True)\n",
        "        self.relu7 = nn.ReLU()\n",
        "        self.fc8 = nn.Linear(in_features=4096, out_features=7, bias=True)\n",
        "\n",
        "    def forward(self, data):\n",
        "        x1 = self.conv1_1(data)\n",
        "        x2 = self.relu1_1(x1)\n",
        "        x3 = self.conv1_2(x2)\n",
        "        x4 = self.relu1_2(x3)\n",
        "        x5 = self.pool1(x4)\n",
        "        x6 = self.conv2_1(x5)\n",
        "        x7 = self.relu2_1(x6)\n",
        "        x8 = self.conv2_2(x7)\n",
        "        x9 = self.relu2_2(x8)\n",
        "        x10 = self.pool2(x9)\n",
        "        x11 = self.conv3_1(x10)\n",
        "        x12 = self.relu3_1(x11)\n",
        "        x13 = self.conv3_2(x12)\n",
        "        x14 = self.relu3_2(x13)\n",
        "        x15 = self.conv3_3(x14)\n",
        "        x16 = self.relu3_3(x15)\n",
        "        x17 = self.pool3(x16)\n",
        "        x18 = self.conv4_1(x17)\n",
        "        x19 = self.relu4_1(x18)\n",
        "        x20 = self.conv4_2(x19)\n",
        "        x21 = self.relu4_2(x20)\n",
        "        x22 = self.conv4_3(x21)\n",
        "        x23 = self.relu4_3(x22)\n",
        "        x24 = self.pool4(x23)\n",
        "        x25 = self.conv5_1(x24)\n",
        "        x26 = self.relu5_1(x25)\n",
        "        x27 = self.conv5_2(x26)\n",
        "        x28 = self.relu5_2(x27)\n",
        "        x29 = self.conv5_3(x28)\n",
        "        x30 = self.relu5_3(x29)\n",
        "        x31 = self.pool5(x30)\n",
        "        x32 = self.fc6(x31) # this is a conv layer, this is the output we need\n",
        "        x33_preflatten = self.relu6(x32)\n",
        "        x33 = x33_preflatten.view(x33_preflatten.size(0), -1)\n",
        "        x34 = self.fc7(x33)\n",
        "        x35 = self.relu7(x34)\n",
        "        prediction = self.fc8(x35)\n",
        "        return prediction\n",
        "\n",
        "\n",
        "def vgg_vd_face_sfew_dag(weights_path=None, **kwargs):\n",
        "    \"\"\"\n",
        "    load imported model instance\n",
        "\n",
        "    Args:\n",
        "        weights_path (str): If set, loads model weights from the given path\n",
        "    \"\"\"\n",
        "    model = Vgg_vd_face_sfew_dag()\n",
        "    if weights_path:\n",
        "        state_dict = torch.load(weights_path)\n",
        "        model.load_state_dict(state_dict)\n",
        "    return model"
      ],
      "metadata": {
        "id": "AQde5xezplaR"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_pretrained = vgg_vd_face_sfew_dag(weights_path=\"/content/gdrive/MyDrive/CV_FER/weights/vgg_vd_face_sfew_dag.pth\") #TODO: save weights"
      ],
      "metadata": {
        "id": "KFbW9mmQp1ah"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class GradientReversalFn(Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, x, alpha):\n",
        "        ctx.alpha = alpha\n",
        "\n",
        "        return x.view_as(x)\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, grad_output):\n",
        "        output = grad_output.neg() * ctx.alpha\n",
        "\n",
        "        return output, None\n",
        "\n",
        "class DANN_VGG(nn.Module):\n",
        "    def __init__(self,\n",
        "                 model_pretrained = model_pretrained,\n",
        "                 num_classes=7,\n",
        "                 dropout_rate = 0.1,\n",
        "                 ):\n",
        "        super(DANN_VGG, self).__init__()\n",
        "        #---------------------Feature Extractor Network---------------#\n",
        "\n",
        "        list_feature_extractor = list(model_pretrained.children())[:-4]\n",
        "        self.feature_extractor = nn.Sequential(*list_feature_extractor)\n",
        "\n",
        "        #---------------------Class Classifier------------------------#\n",
        "\n",
        "        list_class_classifer = list(model_pretrained.children())[-4:]\n",
        "        list_class_classifer.insert(2, nn.Dropout(dropout_rate))\n",
        "        self.class_classifier = nn.Sequential(*list_class_classifer)\n",
        "\n",
        "        #---------------------Domain Classifier-----------------------#\n",
        "\n",
        "        self.domain_classifier = nn.Sequential(nn.ReLU(),\n",
        "                                               nn.Linear(4096, 4096),\n",
        "                                               nn.Dropout(dropout_rate),\n",
        "                                               nn.ReLU(),\n",
        "                                               nn.Linear(4096, 2)\n",
        "                                               )\n",
        "\n",
        "        # Initialize the 4096,4096 to pre-trained\n",
        "        pretrained_weights = model_pretrained.fc7.weight\n",
        "        pretrained_biases = model_pretrained.fc7.bias\n",
        "        with torch.no_grad():\n",
        "          self.domain_classifier[1].weight.copy_(pretrained_weights)\n",
        "          self.domain_classifier[1].bias.copy_(pretrained_biases)\n",
        "\n",
        "\n",
        "    def forward(self, input_data, alpha = 0.0):\n",
        "        features = self.feature_extractor(input_data)\n",
        "        # print(\"features.shape......\", features.shape)\n",
        "        features = features.view(-1,features.size(1))\n",
        "        # print(\"features.shape after view......\", features.shape)\n",
        "\n",
        "        reverse_features = GradientReversalFn.apply(features,alpha)\n",
        "\n",
        "        class_output = self.class_classifier(features)\n",
        "        domain_output = self.domain_classifier(reverse_features)\n",
        "\n",
        "        return class_output, domain_output, features\n"
      ],
      "metadata": {
        "id": "fT_OVfbjp3OY"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Downloading Models"
      ],
      "metadata": {
        "id": "alsERKpBqAlC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "MODEL_DIR = Path(GDRIVE_FOLDER,\"models\") # for drive #TODO :change path\n",
        "non_dann_model_name = 'non_dann_sfew_expw_23_05_wo_se_a.pt'\n",
        "dann_model_name = 'dann_sfew_expw_23_05_wo_se_a.pt'\n",
        "ewc_dann_model_name = 'ewc_dann_sfew_expw_23_05_wo_se_a.pt'"
      ],
      "metadata": {
        "id": "ZSBk2MHyqDdf"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "skeleton_model = DANN_VGG(model_pretrained = model_pretrained, num_classes=7)  # skeleton copy"
      ],
      "metadata": {
        "id": "a2HrxAdyqOD5"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Non-DANN Model"
      ],
      "metadata": {
        "id": "xuhtV84NqRSV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "NON_DANN_MODEL_PATH = Path(os.path.join(MODEL_DIR, f'non_dann_sfew_expw', non_dann_model_name)) #TODO :change path\n",
        "print(NON_DANN_MODEL_PATH)\n",
        "non_dann_model_sfew_expw_inference =  deepcopy(skeleton_model) # DANN_VGG(model_pretrained = model_pretrained, num_classes=7)  # skeleton copy\n",
        "non_dann_model_sfew_expw_inference.load_state_dict(torch.load(NON_DANN_MODEL_PATH,map_location=torch.device(device)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "39m6NaEwqQNa",
        "outputId": "b0195240-69b6-4787-83ae-5bcbac115316"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/CV_FER/models/non_dann_sfew_expw/non_dann_sfew_expw_23_05_wo_se_a.pt\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DANN Model"
      ],
      "metadata": {
        "id": "KOAkcr1mqaCl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DANN_MODEL_PATH = Path(os.path.join(MODEL_DIR, f'dann_sfew_expw', dann_model_name)) #TODO :change path\n",
        "print(DANN_MODEL_PATH)\n",
        "dann_model_sfew_expw_inference =  deepcopy(skeleton_model) # DANN_VGG(model_pretrained = model_pretrained, num_classes=7)  # skeleton copy\n",
        "dann_model_sfew_expw_inference.load_state_dict(torch.load(DANN_MODEL_PATH,map_location=torch.device(device)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BS1xCCwhqapZ",
        "outputId": "a599dd3c-ccae-4d0f-a3c3-7a7a704a31d1"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/CV_FER/models/dann_sfew_expw/dann_sfew_expw_23_05_wo_se_a.pt\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### EWC DANN Model"
      ],
      "metadata": {
        "id": "RpO0VDKwqpFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "EWC_MODEL_PATH = Path(os.path.join(MODEL_DIR, f'ewc_dann_sfew_expw', ewc_dann_model_name)) #TODO :change path\n",
        "print(EWC_MODEL_PATH)\n",
        "ewc_dann_model_sfew_expw_inference =  deepcopy(skeleton_model) # DANN_VGG(model_pretrained = model_pretrained, num_classes=7)  # skeleton copy\n",
        "ewc_dann_model_sfew_expw_inference.load_state_dict(torch.load(EWC_MODEL_PATH,map_location=torch.device(device)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "giuMHRTlqrsV",
        "outputId": "a3972262-f26c-4b1c-b24c-535df62daa74"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/MyDrive/CV_FER/models/ewc_dann_sfew_expw/ewc_dann_sfew_expw_23_05_wo_se_a.pt\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load files\n",
        "\n",
        "1.   List of Samples\n",
        "2.   Dataframe of Samples\n",
        "\n"
      ],
      "metadata": {
        "id": "4iiyiR3SmE4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "GDRIVE_DEMO_FOLDER = '/content/gdrive/MyDrive/da_demo' #TODO :change path\n",
        "DEMO_PATH = Path(GDRIVE_DEMO_FOLDER,'fer')\n",
        "dir_suffix = '27_07' # str(time.strftime(\"%d_%m\"))\n",
        "DEMO_STORAGE_PATH = Path(DEMO_PATH, dir_suffix)\n",
        "list_samples_file_name = 'list_samples.pt'\n",
        "df_samples_file_name = 'df_sample.pt'"
      ],
      "metadata": {
        "id": "_rONYKLBmJuC"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "list_samples_file_path = Path(DEMO_STORAGE_PATH,list_samples_file_name) #TODO :change path\n",
        "list_samples_loaded = torch.load(list_samples_file_path)\n",
        "\n",
        "df_samples_file_path = Path(DEMO_STORAGE_PATH,df_samples_file_name) #TODO :change path\n",
        "df_samples_loaded = torch.load(df_samples_file_path)"
      ],
      "metadata": {
        "id": "H54KYbFMl2c_"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Condition 1: Images that are predicted same in non_dann, dann, dann_ewc, ground truth - 30"
      ],
      "metadata": {
        "id": "Aft1eJcJnOuI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "condition_1 = (df_samples_loaded['gt_emotion'] == df_samples_loaded['p_emotion_non_dann']) & (df_samples_loaded['gt_emotion'] == df_samples_loaded['p_emotion_dann']) & ( df_samples_loaded['gt_emotion'] == df_samples_loaded['p_emotion_ewc_dann'])"
      ],
      "metadata": {
        "id": "UaS2od1nnEjT"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_condition_1 = df_samples_loaded[condition_1].reset_index(drop=True)"
      ],
      "metadata": {
        "id": "liO7Nt82nkJM"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Condition 2: Images that are nok in non_dann, ok in dann and dann_ewc - 20"
      ],
      "metadata": {
        "id": "02RjVbD0nxFE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "condition_2 = (df_samples_loaded['gt_emotion'] != df_samples_loaded['p_emotion_non_dann']) & (df_samples_loaded['gt_emotion'] == df_samples_loaded['p_emotion_dann']) & ( df_samples_loaded['gt_emotion'] == df_samples_loaded['p_emotion_ewc_dann'])\n",
        "df_condition_2 = df_samples_loaded[condition_2].reset_index(drop=True)"
      ],
      "metadata": {
        "id": "MGb4Ez9inzxi"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Condition 3. Images that are nok in non_dann, nok in dann and ok in dann_ewc - 27\n"
      ],
      "metadata": {
        "id": "vLHelaZuoM8j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "condition_3 = (df_samples_loaded['gt_emotion'] != df_samples_loaded['p_emotion_non_dann']) & (df_samples_loaded['gt_emotion'] != df_samples_loaded['p_emotion_dann']) & ( df_samples_loaded['gt_emotion'] == df_samples_loaded['p_emotion_ewc_dann'])"
      ],
      "metadata": {
        "id": "BabBozAFoNQ3"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_condition_3 = df_samples_loaded[condition_3].reset_index(drop=True)"
      ],
      "metadata": {
        "id": "9l2mAMGToQmA"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Condition 4. Images that are nok in non_dann, dann_ewc but ok in dann - 24"
      ],
      "metadata": {
        "id": "Wd2kjmyHokpl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "condition_4 = (df_samples_loaded['gt_emotion'] != df_samples_loaded['p_emotion_non_dann']) & (df_samples_loaded['gt_emotion'] == df_samples_loaded['p_emotion_dann']) & ( df_samples_loaded['gt_emotion'] != df_samples_loaded['p_emotion_ewc_dann'])"
      ],
      "metadata": {
        "id": "eL17TAmGojzG"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_condition_4 = df_samples_loaded[condition_4].reset_index(drop=True)"
      ],
      "metadata": {
        "id": "Bi0lVcwNozue"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Condition 5. Images which are non_ok on all 3 - 40"
      ],
      "metadata": {
        "id": "U-N4Ew66o4CA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "condition_5 = (df_samples_loaded['gt_emotion'] != df_samples_loaded['p_emotion_non_dann']) & (df_samples_loaded['gt_emotion'] != df_samples_loaded['p_emotion_dann']) & ( df_samples_loaded['gt_emotion'] != df_samples_loaded['p_emotion_ewc_dann'])"
      ],
      "metadata": {
        "id": "GiChBu3Eo4Zd"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df_condition_5 = df_samples_loaded[condition_5].reset_index(drop=True)"
      ],
      "metadata": {
        "id": "mvOY612jo_sX"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataloader"
      ],
      "metadata": {
        "id": "6xweU5fQsWoA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "# Define the labels map\n",
        "labels_map = {\n",
        "    \"0\": \"angry\",\n",
        "    \"1\": \"disgust\",\n",
        "    \"2\": \"fear\",\n",
        "    \"3\": \"happy\",\n",
        "    \"4\": \"sad\",\n",
        "    \"5\": \"surprise\",\n",
        "    \"6\": \"neutral\"\n",
        "}\n",
        "\n",
        "cpu_batch_size = 8\n",
        "\n",
        "# List of labels\n",
        "labels = list(labels_map.values())\n",
        "\n",
        "# Create the one-hot encoding matrix\n",
        "label_matrix = torch.eye(len(labels))\n",
        "\n",
        "# Function to get the one-hot vector for a specific emotion\n",
        "def get_one_hot_vector(emotion, labels = labels, label_matrix= label_matrix):\n",
        "    if emotion in labels:\n",
        "        idx = labels.index(emotion)\n",
        "        return label_matrix[idx]\n",
        "    else:\n",
        "        raise ValueError(f\"Emotion '{emotion}' not found in labels.\")"
      ],
      "metadata": {
        "id": "FLF7k3cNvMN7"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class CustomImageDataset(Dataset):\n",
        "    def __init__(self, dataframe,\n",
        "                 transform=None,\n",
        "                 image_file_colname = 'image',\n",
        "                 race_colname  = 'gt_race',\n",
        "                 gt_emotion_colname  = 'gt_emotion',\n",
        "                 image_pil_colname = 'image_pil'):\n",
        "        self.dataframe = dataframe.reset_index(drop=True)\n",
        "        self.basic_transform = transforms.Compose([transforms.Resize(224),\n",
        "                                                   transforms.ToTensor()])\n",
        "        self.transform = transform\n",
        "        self.image_file_colname = image_file_colname\n",
        "        self.race_colname = race_colname\n",
        "        self.gt_emotion_colname = gt_emotion_colname\n",
        "        self.image_pil_colname = image_pil_colname\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataframe)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "\n",
        "        img_filename = self.dataframe.loc[idx, self.image_file_colname]\n",
        "        race = self.dataframe.loc[idx, self.race_colname]\n",
        "        emotion = self.dataframe.loc[idx, self.gt_emotion_colname]\n",
        "        emotion_one_hot = get_one_hot_vector(emotion.lower()) # emotion.lower() to match the labels\n",
        "        image = self.dataframe.loc[idx, self.image_pil_colname] # pil image\n",
        "        cropped_image = mtcnn(image) # mtcnn takes in PIL, returns tensor in cropped image 3x224x224\n",
        "\n",
        "        if self.transform:\n",
        "          image_transformed = self.transform(image) # original image, this converts PIL into tensor\n",
        "        else:\n",
        "          image_transformed = self.basic_transform(image) # original image, this converts PIL into tensor\n",
        "\n",
        "        if cropped_image is None: #error in cropping\n",
        "          # in case of error, original image is returned\n",
        "          return image_transformed, emotion_one_hot, image_transformed, race\n",
        "        else: # cropping went ok\n",
        "          # cropped image alongside original image is returned, there is no transform on cropped image.\n",
        "          cropped_image = (cropped_image + 1) / 2 # changing form -1,1 to 0,1\n",
        "          return cropped_image, emotion_one_hot, image_transformed, race\n"
      ],
      "metadata": {
        "id": "nUXD9mbbvQwJ"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset and Dataloader for Conditions 1 to 5"
      ],
      "metadata": {
        "id": "fg0xuEkfwJOm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
        "    transforms.ToTensor(),          # Convert images to PyTorch tensors\n",
        "])"
      ],
      "metadata": {
        "id": "qGnG9T_VwP25"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# condition 1\n",
        "dataset_condition_1 = CustomImageDataset(dataframe= df_condition_1, transform=transform)\n",
        "dataloader_condition_1 = DataLoader(dataset_condition_1, batch_size=cpu_batch_size, shuffle=True)\n",
        "# condition 2\n",
        "dataset_condition_2 = CustomImageDataset(dataframe= df_condition_2, transform=transform)\n",
        "dataloader_condition_2 = DataLoader(dataset_condition_2, batch_size=cpu_batch_size, shuffle=True)\n",
        "# condition 3\n",
        "dataset_condition_3 = CustomImageDataset(dataframe= df_condition_3, transform=transform)\n",
        "dataloader_condition_3 = DataLoader(dataset_condition_3, batch_size=cpu_batch_size, shuffle=True)\n",
        "# condition 4\n",
        "dataset_condition_4 = CustomImageDataset(dataframe= df_condition_4, transform=transform)\n",
        "dataloader_condition_4 = DataLoader(dataset_condition_4, batch_size=cpu_batch_size, shuffle=True)\n",
        "# condition 5\n",
        "dataset_condition_5 = CustomImageDataset(dataframe= df_condition_5, transform=transform)\n",
        "dataloader_condition_5 = DataLoader(dataset_condition_5, batch_size=cpu_batch_size, shuffle=True)"
      ],
      "metadata": {
        "id": "BTxTk6r2whHp"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def show_image(image_tensor):\n",
        "#   image_tensor_np = image_tensor.numpy()\n",
        "#   # print(\"before\", image_tensor_np.shape)\n",
        "#   image_tensor_np = image_tensor_np.squeeze()\n",
        "#   # print(\"post squeeze\", image_tensor_np.shape)\n",
        "\n",
        "#   # Since Matplotlib expects images in (H, W, C) format, transpose the tensor from (C, H, W)\n",
        "#   image_tensor_np = np.transpose(image_tensor_np, (1, 2, 0))\n",
        "#   # print(\"after\", image_tensor_np.shape)\n",
        "\n",
        "#   # Display the image using Matplotlib\n",
        "#   plt.imshow(image_tensor_np)\n",
        "#   plt.axis('off')  # Turn off axis labels\n",
        "#   plt.show()\n",
        "#   # img\n",
        "\n",
        "# # Example of iterating through the DataLoader\n",
        "# for batch in dataloader_condition_5:\n",
        "#     cropped_images, emotions, images, races = batch\n",
        "#     cropped_image, emotion, image, race = cropped_images[0], emotions[0], images[0], races[0]\n",
        "#     show_image (image)\n",
        "#     show_image(cropped_image)\n",
        "#     print(emotion , labels_map[str(emotion.argmax().item())])\n",
        "#     print(race)\n",
        "#     break"
      ],
      "metadata": {
        "id": "VjG13E1Ozg55"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# UI"
      ],
      "metadata": {
        "id": "PlUdIUlFxyQi"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get images function for all conditions"
      ],
      "metadata": {
        "id": "ic1giXqPyAJ-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform_to_pil  = transforms.ToPILImage()\n",
        "emotion_labels = [label.capitalize() for label in list(labels_map.values())]\n",
        "\n",
        "def get_images(dataloader = dataloader_condition_1):\n",
        "    cropped_images, emotions, images, races = next(iter(dataloader))\n",
        "\n",
        "    list_pil_cropped_images = [transform_to_pil(cropped_img) for cropped_img in cropped_images]\n",
        "    list_pil_images = [transform_to_pil(img) for img in images]\n",
        "    list_emotions = list(emotions) # list of one hot tensort\n",
        "    list_emotions = [ emotion_labels[torch.argmax(emotion).item()] for emotion in emotions]\n",
        "\n",
        "    return list_pil_cropped_images, list_emotions, list_pil_images, list(races)"
      ],
      "metadata": {
        "id": "U6rjZ4T9x0BF"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_images_condition_2():\n",
        "    return get_images(dataloader_condition_2)\n",
        "\n",
        "def get_images_condition_3():\n",
        "    return get_images(dataloader_condition_3)\n",
        "\n",
        "def get_images_condition_4():\n",
        "    return get_images(dataloader_condition_4)\n",
        "\n",
        "def get_images_condition_5():\n",
        "    return get_images(dataloader_condition_5)"
      ],
      "metadata": {
        "id": "lvJuFzzhx_vY"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "get_images_condition_5()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uWFFaeKk0PGP",
        "outputId": "eed36fce-ea7d-4473-9d1b-a0f23d3fbffc"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "([<PIL.Image.Image image mode=RGB size=224x224>,\n",
              "  <PIL.Image.Image image mode=RGB size=224x224>,\n",
              "  <PIL.Image.Image image mode=RGB size=224x224>,\n",
              "  <PIL.Image.Image image mode=RGB size=224x224>,\n",
              "  <PIL.Image.Image image mode=RGB size=224x224>,\n",
              "  <PIL.Image.Image image mode=RGB size=224x224>,\n",
              "  <PIL.Image.Image image mode=RGB size=224x224>,\n",
              "  <PIL.Image.Image image mode=RGB size=224x224>],\n",
              " ['Disgust',\n",
              "  'Surprise',\n",
              "  'Surprise',\n",
              "  'Neutral',\n",
              "  'Surprise',\n",
              "  'Neutral',\n",
              "  'Neutral',\n",
              "  'Angry'],\n",
              " [<PIL.Image.Image image mode=RGB size=224x224>,\n",
              "  <PIL.Image.Image image mode=RGB size=224x224>,\n",
              "  <PIL.Image.Image image mode=RGB size=224x224>,\n",
              "  <PIL.Image.Image image mode=RGB size=224x224>,\n",
              "  <PIL.Image.Image image mode=RGB size=224x224>,\n",
              "  <PIL.Image.Image image mode=RGB size=224x224>,\n",
              "  <PIL.Image.Image image mode=RGB size=224x224>,\n",
              "  <PIL.Image.Image image mode=RGB size=224x224>],\n",
              " ['Indian',\n",
              "  'Indian',\n",
              "  'African',\n",
              "  'Asian',\n",
              "  'Latino',\n",
              "  'Indian',\n",
              "  'Middle Eastern',\n",
              "  'White'])"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Classify Images All models"
      ],
      "metadata": {
        "id": "-Q13Qms6yoqA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "emotion_labels = [label.capitalize() for label in list(labels_map.values())]\n",
        "\n",
        "def classify_image_all_models(input_image):\n",
        "    # print(\"-------\")\n",
        "    # print(input_image)\n",
        "    # print(type(input_image))\n",
        "    # print(\"-------\")\n",
        "\n",
        "    image_transforms =  transforms.Compose([\n",
        "                                  transforms.Resize((224,224)),\n",
        "                                  transforms.ToTensor()\n",
        "                                  ])\n",
        "    transformed_image = image_transforms(input_image)\n",
        "    image_tensor = transformed_image.to(device).unsqueeze(0)\n",
        "\n",
        "    list_confidences = []\n",
        "    for model in [non_dann_model_sfew_expw_inference, dann_model_sfew_expw_inference, ewc_dann_model_sfew_expw_inference]:\n",
        "        model = model.to(device)\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            logits, _ , _ = model((image_tensor*255))\n",
        "            output = F.softmax(logits.view(-1), dim = -1)\n",
        "\n",
        "            confidences = [(emotion_labels[i], float(output[i])) for i in range(len(emotion_labels))]\n",
        "            confidences.sort(key=lambda x: x[1], reverse=True)\n",
        "            confidences = OrderedDict(confidences[:2])\n",
        "            label = torch.argmax(output).item()\n",
        "            list_confidences.append(confidences)\n",
        "\n",
        "    return list_confidences[0], list_confidences[1], list_confidences[2]"
      ],
      "metadata": {
        "id": "qX8dY20oyuFZ"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Gradio UI"
      ],
      "metadata": {
        "id": "I_PhNEc3y15Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "with gr.Blocks() as demo:\n",
        "    with gr.Tab(\"Introduction\"):\n",
        "        gr.Markdown(\"## Domain Adaptation in Deep Networks - Demonstration\")\n",
        "        gr.Markdown(\n",
        "            '''\n",
        "            Source - SFEW2.0\n",
        "            -------\n",
        "            - SFEW2.0 Details\n",
        "\n",
        "            '''\n",
        "        )\n",
        "        gr.Markdown(\n",
        "            '''\n",
        "            Target - ExpW\n",
        "            -------\n",
        "            - ExpW Details\n",
        "\n",
        "            '''\n",
        "        )\n",
        "    ################################################\n",
        "    with gr.Tab(\"Case 1: {OK : Non DANN, DANN, EWC DANN}\") as tabs1:\n",
        "        imgs = gr.State()\n",
        "\n",
        "        # gallery = gr.Gallery(allow_preview=False)\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                gallery = gr.Gallery(allow_preview=True, rows=2, columns=2)\n",
        "\n",
        "            with gr.Column(scale=1):\n",
        "                with gr.Row():\n",
        "                    cropped_image_display = gr.Image(label=\"Cropped Image\", type=\"pil\", height=224, width=224)\n",
        "                with gr.Row():\n",
        "                    button_classify_C1 = gr.Button(\"Click Button to Predict Emotion\", visible=True, size='sm')\n",
        "                with gr.Row():\n",
        "                    # selected = gr.Number(show_label=False)\n",
        "                    selected = gr.Textbox(label=\"Ground Truth Emotion\", visible=False)\n",
        "                    txtbox_race = gr.Textbox(label=\"Race\", visible=False)\n",
        "\n",
        "            with gr.Column(scale=1):\n",
        "                with gr.Row():\n",
        "                    label_classify_non_dann = gr.Label(label=\"NON DANN Predicted label\", num_top_classes=2, visible=True)\n",
        "                with gr.Row():\n",
        "                    label_classify_dann = gr.Label(label=\"DANN Predicted label\", num_top_classes=2, visible=True)\n",
        "                with gr.Row():\n",
        "                    label_classify_ewc = gr.Label(label=\"EWC DANN Predicted label\", num_top_classes=2, visible=True)\n",
        "\n",
        "        cropped_images, list_emotions, big_images, list_races = get_images()\n",
        "\n",
        "        def get_big_images():\n",
        "            return big_images, big_images\n",
        "\n",
        "        def get_select_index(evt: gr.SelectData):\n",
        "            # return evt.index, cropped_images[evt.index]\n",
        "            return list_emotions[evt.index], gr.update(visible=True), cropped_images[evt.index], list_races[evt.index], gr.update(visible=True)\n",
        "\n",
        "        refresh_case1 = gr.Checkbox(visible=False)\n",
        "\n",
        "        def refresh_gallery():\n",
        "            return get_big_images()\n",
        "\n",
        "        refresh_case1.change(refresh_gallery, None, [gallery, imgs])\n",
        "        gallery.select(get_select_index, None, [selected, selected, cropped_image_display, txtbox_race, txtbox_race])\n",
        "        button_classify_C1.click(fn=classify_image_all_models, inputs=[cropped_image_display],\n",
        "                                 outputs=[label_classify_non_dann, label_classify_dann, label_classify_ewc])\n",
        "\n",
        "    ################################################\n",
        "    with gr.Tab(\"Case 2:{ NOK Non DANN}, {OK : DANN, EWC DANN}\") as tabs2:\n",
        "        imgs_2 = gr.State()\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                gallery_2 = gr.Gallery(allow_preview=True, rows=2, columns=2)\n",
        "\n",
        "            with gr.Column(scale=1):\n",
        "                with gr.Row():\n",
        "                    cropped_image_display_2 = gr.Image(label=\"Cropped Image\", type=\"pil\", height=224, width=224)\n",
        "                with gr.Row():\n",
        "                    button_classify_C2 = gr.Button(\"Click Button to Predict Emotion\", visible=True, size='sm')\n",
        "                with gr.Row():\n",
        "                    selected_2 = gr.Textbox(label=\"Ground Truth Emotion\", visible=False)\n",
        "                    txtbox_race_2 = gr.Textbox(label=\"Race\", visible=False)\n",
        "\n",
        "            with gr.Column(scale=1):\n",
        "                with gr.Row():\n",
        "                    label_classify_non_dann_2 = gr.Label(label=\"NON DANN Predicted label\", num_top_classes=2, visible=True)\n",
        "                with gr.Row():\n",
        "                    label_classify_dann_2 = gr.Label(label=\"DANN Predicted label\", num_top_classes=2, visible=True)\n",
        "                with gr.Row():\n",
        "                    label_classify_ewc_2 = gr.Label(label=\"EWC DANN Predicted label\", num_top_classes=2, visible=True)\n",
        "\n",
        "        cropped_images_2, list_emotions_2, big_images_2, list_races_2 = get_images_condition_2()\n",
        "\n",
        "        def get_big_images_2():\n",
        "            return big_images_2, big_images_2\n",
        "\n",
        "        def get_select_index_2(evt: gr.SelectData):\n",
        "            # return evt.index, cropped_images[evt.index]\n",
        "            return list_emotions_2[evt.index], gr.update(visible=True), cropped_images_2[evt.index], list_races_2[evt.index], gr.update(visible=True)\n",
        "\n",
        "        refresh_case2 = gr.Checkbox(visible=False)\n",
        "\n",
        "        def refresh_gallery_2():\n",
        "            return get_big_images_2()\n",
        "\n",
        "        refresh_case2.change(refresh_gallery_2, None, [gallery_2, imgs_2])\n",
        "        gallery_2.select(get_select_index_2, None, [selected_2, selected_2, cropped_image_display_2, txtbox_race_2, txtbox_race_2])\n",
        "        button_classify_C2.click(fn=classify_image_all_models, inputs=[cropped_image_display_2],\n",
        "                                 outputs=[label_classify_non_dann_2, label_classify_dann_2, label_classify_ewc_2])\n",
        "\n",
        "    ################################################\n",
        "    with gr.Tab(\"Case 3:{ NOK: Non DANN, DANN}, {OK:EWC DANN}\")as tabs3:\n",
        "        imgs_3 = gr.State()\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                gallery_3 = gr.Gallery(allow_preview=True, rows=2, columns=2)\n",
        "\n",
        "            with gr.Column(scale=1):\n",
        "                with gr.Row():\n",
        "                    cropped_image_display_3 = gr.Image(label=\"Cropped Image\", type=\"pil\", height=224, width=224)\n",
        "                with gr.Row():\n",
        "                    button_classify_C3 = gr.Button(\"Click Button to Predict Emotion\", visible=True, size='sm')\n",
        "                with gr.Row():\n",
        "                    selected_3 = gr.Textbox(label=\"Ground Truth Emotion\", visible=False)\n",
        "                    txtbox_race_3 = gr.Textbox(label=\"Race\", visible=False)\n",
        "\n",
        "            with gr.Column(scale=1):\n",
        "                with gr.Row():\n",
        "                    label_classify_non_dann_3 = gr.Label(label=\"NON DANN Predicted label\", num_top_classes=2, visible=True)\n",
        "                with gr.Row():\n",
        "                    label_classify_dann_3 = gr.Label(label=\"DANN Predicted label\", num_top_classes=2, visible=True)\n",
        "                with gr.Row():\n",
        "                    label_classify_ewc_3 = gr.Label(label=\"EWC DANN Predicted label\", num_top_classes=2, visible=True)\n",
        "\n",
        "        cropped_images_3, list_emotions_3, big_images_3, list_races_3 = get_images_condition_3()\n",
        "\n",
        "        def get_big_images_3():\n",
        "            return big_images_3, big_images_3\n",
        "\n",
        "        def get_select_index_3(evt: gr.SelectData):\n",
        "            # return evt.index, cropped_images[evt.index]\n",
        "            return list_emotions_3[evt.index], gr.update(visible=True), cropped_images_3[evt.index], list_races_3[evt.index], gr.update(visible=True)\n",
        "\n",
        "        refresh_case3 = gr.Checkbox(visible=False)\n",
        "\n",
        "        def refresh_gallery_3():\n",
        "            return get_big_images_3()\n",
        "\n",
        "        refresh_case3.change(refresh_gallery_3, None, [gallery_3, imgs_3])\n",
        "        gallery_3.select(get_select_index_3, None, [selected_3, selected_3, cropped_image_display_3, txtbox_race_3, txtbox_race_3])\n",
        "        button_classify_C3.click(fn=classify_image_all_models, inputs=[cropped_image_display_3],\n",
        "                                 outputs=[label_classify_non_dann_3, label_classify_dann_3, label_classify_ewc_3])\n",
        "\n",
        "    ################################################\n",
        "    with gr.Tab(\"Case 4:{ NOK: Non DANN, EWC DANN}, {OK:DANN}\") as tabs4:\n",
        "        imgs_4 = gr.State()\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                gallery_4 = gr.Gallery(allow_preview=True, rows=2, columns=2)\n",
        "\n",
        "            with gr.Column(scale=1):\n",
        "                with gr.Row():\n",
        "                    cropped_image_display_4 = gr.Image(label=\"Cropped Image\", type=\"pil\", height=224, width=224)\n",
        "                with gr.Row():\n",
        "                    button_classify_C4 = gr.Button(\"Click Button to Predict Emotion\", visible=True, size='sm')\n",
        "                with gr.Row():\n",
        "                    selected_4 = gr.Textbox(label=\"Ground Truth Emotion\", visible=False)\n",
        "                    txtbox_race_4 = gr.Textbox(label=\"Race\", visible=False)\n",
        "\n",
        "            with gr.Column(scale=1):\n",
        "                with gr.Row():\n",
        "                    label_classify_non_dann_4 = gr.Label(label=\"NON DANN Predicted label\", num_top_classes=2, visible=True)\n",
        "                with gr.Row():\n",
        "                    label_classify_dann_4 = gr.Label(label=\"DANN Predicted label\", num_top_classes=2, visible=True)\n",
        "                with gr.Row():\n",
        "                    label_classify_ewc_4 = gr.Label(label=\"EWC DANN Predicted label\", num_top_classes=2, visible=True)\n",
        "\n",
        "        cropped_images_4, list_emotions_4, big_images_4, list_races_4 = get_images_condition_4()\n",
        "\n",
        "        def get_big_images_4():\n",
        "            return big_images_4, big_images_4\n",
        "\n",
        "        def get_select_index_4(evt: gr.SelectData):\n",
        "            return list_emotions_4[evt.index], gr.update(visible=True), cropped_images_4[evt.index], list_races_4[evt.index], gr.update(visible=True)\n",
        "\n",
        "        refresh_case4 = gr.Checkbox(visible=False)\n",
        "\n",
        "        def refresh_gallery_4():\n",
        "            return get_big_images_4()\n",
        "\n",
        "        refresh_case4.change(refresh_gallery_4, None, [gallery_4, imgs_4])\n",
        "        gallery_4.select(get_select_index_4, None, [selected_4, selected_4, cropped_image_display_4, txtbox_race_4, txtbox_race_4])\n",
        "        button_classify_C4.click(fn=classify_image_all_models, inputs=[cropped_image_display_4],\n",
        "                                 outputs=[label_classify_non_dann_4, label_classify_dann_4, label_classify_ewc_4])\n",
        "\n",
        "    ################################################\n",
        "    with gr.Tab(\"Case 5:{ NOK: Non DANN, DANN, EWC DANN}\") as tabs5:\n",
        "        imgs_5 = gr.State()\n",
        "\n",
        "        with gr.Row():\n",
        "            with gr.Column(scale=1):\n",
        "                gallery_5 = gr.Gallery(allow_preview=True, rows=2, columns=2)\n",
        "\n",
        "            with gr.Column(scale=1):\n",
        "                with gr.Row():\n",
        "                    cropped_image_display_5 = gr.Image(label=\"Cropped Image\", type=\"pil\", height=224, width=224)\n",
        "                with gr.Row():\n",
        "                    button_classify_C5 = gr.Button(\"Click Button to Predict Emotion\", visible=True, size='sm')\n",
        "                with gr.Row():\n",
        "                    selected_5 = gr.Textbox(label=\"Ground Truth Emotion\", visible=False)\n",
        "                    txtbox_race_5 = gr.Textbox(label=\"Race\", visible=False)\n",
        "\n",
        "            with gr.Column(scale=1):\n",
        "                with gr.Row():\n",
        "                    label_classify_non_dann_5 = gr.Label(label=\"NON DANN Predicted label\", num_top_classes=2, visible=True)\n",
        "                with gr.Row():\n",
        "                    label_classify_dann_5 = gr.Label(label=\"DANN Predicted label\", num_top_classes=2, visible=True)\n",
        "                with gr.Row():\n",
        "                    label_classify_ewc_5 = gr.Label(label=\"EWC DANN Predicted label\", num_top_classes=2, visible=True)\n",
        "\n",
        "        cropped_images_5, list_emotions_5, big_images_5, list_races_5 = get_images_condition_5()\n",
        "\n",
        "        def get_big_images_5():\n",
        "            return big_images_5, big_images_5\n",
        "\n",
        "        def get_select_index_5(evt: gr.SelectData):\n",
        "            return list_emotions_5[evt.index], gr.update(visible=True), cropped_images_5[evt.index], list_races_5[evt.index], gr.update(visible=True)\n",
        "\n",
        "        refresh_case5 = gr.Checkbox(visible=False)\n",
        "\n",
        "        def refresh_gallery_5():\n",
        "            return get_big_images_5()\n",
        "\n",
        "        refresh_case5.change(refresh_gallery_5, None, [gallery_5, imgs_5])\n",
        "        gallery_5.select(get_select_index_5, None, [selected_5, selected_5, cropped_image_display_5, txtbox_race_5, txtbox_race_5])\n",
        "        button_classify_C5.click(fn=classify_image_all_models, inputs=[cropped_image_display_5],\n",
        "                                 outputs=[label_classify_non_dann_5, label_classify_dann_5, label_classify_ewc_5])\n",
        "\n",
        "\n",
        "    ################################################\n",
        "\n",
        "\n",
        "    def refresh_tab(): return True\n",
        "    def refresh_tab2(): return True\n",
        "    def refresh_tab3(): return True\n",
        "    def refresh_tab4(): return True\n",
        "    def refresh_tab5(): return True\n",
        "\n",
        "\n",
        "    with gr.Row(visible=True):\n",
        "        refresh_tab_1 = gr.Button(value=\"refresh Case 1\", visible=True)\n",
        "        refresh_tab_2 = gr.Button(value=\"refresh Case 2\", visible=True)\n",
        "        refresh_tab_3 = gr.Button(value=\"refresh Case 3\", visible=True)\n",
        "        refresh_tab_4 = gr.Button(value=\"refresh Case 4\", visible=True)\n",
        "        refresh_tab_5 = gr.Button(value=\"refresh Case 5\", visible=True)\n",
        "\n",
        "    refresh_tab_1.click(refresh_tab, [], [refresh_case1])\n",
        "    refresh_tab_2.click(refresh_tab, [], [refresh_case2])\n",
        "    refresh_tab_3.click(refresh_tab, [], [refresh_case3])\n",
        "    refresh_tab_4.click(refresh_tab, [], [refresh_case4])\n",
        "    refresh_tab_5.click(refresh_tab, [], [refresh_case5])\n",
        "\n",
        "demo.launch(debug=True)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 869
        },
        "id": "f9_LysTNy3mA",
        "outputId": "87e2012a-3f38-47c3-abcd-93378b1392ab"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setting queue=True in a Colab notebook requires sharing enabled. Setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. This cell will run indefinitely so that you can see errors and logs. To turn off, set debug=False in launch().\n",
            "Running on public URL: https://2179ff573341317965.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from Terminal to deploy to Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://2179ff573341317965.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------\n",
            "<PIL.Image.Image image mode=RGB size=224x224 at 0x79F635F85BA0>\n",
            "<class 'PIL.Image.Image'>\n",
            "-------\n",
            "-------\n",
            "<PIL.Image.Image image mode=RGB size=224x224 at 0x79F635F86AD0>\n",
            "<class 'PIL.Image.Image'>\n",
            "-------\n",
            "-------\n",
            "<PIL.Image.Image image mode=RGB size=224x224 at 0x79F635F86D70>\n",
            "<class 'PIL.Image.Image'>\n",
            "-------\n",
            "Keyboard interruption in main thread... closing server.\n",
            "Killing tunnel 127.0.0.1:7860 <> https://2179ff573341317965.gradio.live\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    }
  ]
}